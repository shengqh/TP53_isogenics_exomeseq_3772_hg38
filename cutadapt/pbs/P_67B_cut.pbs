#!/bin/bash
#SBATCH --mail-user=quanhu.sheng.1@vumc.org
#SBATCH --mail-type=FAIL
#SBATCH --nodes=1
#SBATCH --ntasks=2
#SBATCH --time=14:00:00
#SBATCH --mem=22G
#SBATCH --constraint=haswell
#SBATCH -o /scratch/cqs/shengq2/jennifer/20200407_lindsay_exomeseq_3772_hg38/cutadapt/log/P_67B_cut.log

if [ -n "${SLURM_JOB_ID}" ] ; then
  smemwatch -k 99 -d 50 $$ &
fi






set -o pipefail

cd '/scratch/cqs/shengq2/jennifer/20200407_lindsay_exomeseq_3772_hg38/cutadapt/result'


if [[ !(1 -eq $1) ]]; then
  if [[ ( -s P_67B_clipped.1.fastq.gz ) || ( -d P_67B_clipped.1.fastq.gz ) ]]; then
    echo job has already been done. if you want to do again, delete /scratch/cqs/shengq2/jennifer/20200407_lindsay_exomeseq_3772_hg38/cutadapt/result/P_67B_clipped.1.fastq.gz and submit job again.
    exit 0
  fi
fi

echo Trimmer::Cutadapt_start=`date`
echo working in /scratch/cqs/shengq2/jennifer/20200407_lindsay_exomeseq_3772_hg38/cutadapt/result ...
 

export R_LIBS=
export PYTHONPATH=
export JAVA_HOME=
 


singularity exec -c -B /gpfs23,/scratch,/gpfs52,/data,/home,/tmp -e /data/cqs/softwares/singularity/cqs-exomeseq.simg  bash /scratch/cqs/shengq2/jennifer/20200407_lindsay_exomeseq_3772_hg38/cutadapt/pbs/P_67B_cut.pbs.sh 

echo Trimmer::Cutadapt_end=`date`

exit 0

